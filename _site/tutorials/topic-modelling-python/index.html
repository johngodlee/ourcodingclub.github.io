<!DOCTYPE html>
<html lang="en-GB">
	<head>
    	<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Topic Modelling in Python</title>
<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- CSS -->
<link rel="stylesheet" href="/css/main.css"/>
<link rel="stylesheet" href="/css/owlcarousel/owl.carousel.css">
<link rel="stylesheet" href="/css/owlcarousel/owl.theme.default.css">

<!-- JS -->
<script src="https://use.fontawesome.com/4dd22df1f8.js"></script>
<script src="https://code.jquery.com/jquery-1.12.4.js"></script>
<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
<script src="/scripts/accordion.js"></script>
<script src="/scripts/jquery.counterup.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/waypoints/2.0.5/waypoints.min.js"></script>
<script src="/scripts/ticker.js"></script>

	</head>
	<body>
    	<header class="header">
	<div class="navigation-bar">
		<div id="navigation-container">
			
				<a class="logo" href="/">
  <img src="/assets/img/logos/logo_stack.svg" alt="Coding Club logo">
</a>

			
			<nav>
				<label for="hamburger">☰</label>
				<input type="checkbox" id="hamburger">
				<ul>
					
						
					<li class="item item-nav">
						<a href="/">Home</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/tutorials.html">Tutorials</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/course.html">Course</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/team.html">Team</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/involve.html">Get involved</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/links.html">Links</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/contact.html">Contact</a>
					</li>
					
				</ul>
			</nav>
		</div>
	</div>
</header>

			<div class="block">
  <center><img src="/img/tutheader-topic-modelling-python.png" alt="Img"></center>
</div>

<h3 id="tutorial-aims">Tutorial aims:</h3>

<h4 id="-1-introduction-and-getting-started"><a href="#introduction"> 1. Introduction and getting started</a></h4>

<h4 id="-2-exploring-text-datasets"><a href="#eda"> 2. Exploring text datasets</a></h4>

<h4 id="-3-extracting-substrings-with-regular-expressions"><a href="#who_what"> 3. Extracting substrings with regular expressions</a></h4>

<h4 id="-4-finding-keyword-correlations-in-text-data"><a href="#text_corr"> 4. Finding keyword correlations in text data</a></h4>

<h4 id="-5-introduction-to-topic-modelling"><a href="#top_mod"> 5. Introduction to topic modelling</a></h4>

<h4 id="-6-cleaning-text-data"><a href="#clean"> 6. Cleaning text data</a></h4>

<h4 id="-7-applying-topic-modelling"><a href="#apply"> 7. Applying topic modelling</a></h4>

<h4 id="-8-bonus-exercises"><a href="#bonus"> 8. Bonus exercises</a></h4>

<p><a name="introduction"></a></p>
<h1 id="introduction">Introduction</h1>
<p>In this tutorial we are going to be performing topic modelling on twitter data to find what people are tweeting about in relation to climate change. From a sample dataset we will clean the text data and explore what popular hashtags are being used, who is being tweeted at and retweeted, and finally we will use two unsupervised machine learning algorithms, specifically latent dirichlet allocation (LDA) and non-negative matrix factorisation (NMF), to explore the topics of the tweets in full.</p>

<hr>

<p><strong>Prerequisites</strong></p>
<ul>
  <li>In order to do this tutorial, you should be comfortable with basic Python, the <code class="language-plaintext highlighter-rouge">pandas</code> and <code class="language-plaintext highlighter-rouge">numpy</code> packages and should be comfortable with making and interpreting plots.</li>
  <li>You will need to have the following packages installed : <code class="language-plaintext highlighter-rouge">numpy</code>, <code class="language-plaintext highlighter-rouge">pandas</code>, <code class="language-plaintext highlighter-rouge">seaborn</code>, <code class="language-plaintext highlighter-rouge">matplotlib</code>, <code class="language-plaintext highlighter-rouge">sklearn</code>, <code class="language-plaintext highlighter-rouge">nltk</code>
</li>
</ul>

<h3 id="getting-started">Getting Started</h3>

<p>Twitter is a fantastic source of data for a social scientist, with over 8,000 tweets sent per second. The tweets that millions of users send can be downloaded and analysed to try and investigate mass opinion on particular issues. This can be as basic as looking for keywords and phrases like <em>‘marmite is bad’</em> or <em>‘marmite is good’</em> or can be more advanced, aiming to discover general topics (not just marmite related ones) contained in a dataset. We are going to do a bit of both.</p>

<p>The first thing we will do is to get you set up with the data.</p>

<h3 id="the-data-you-need-to-complete-this-tutorial-can-be-downloaded-from-this-repository-click-on-clonedownloaddownload-zip-and-unzip-the-folder-or-clone-the-repository-to-your-own-github-account">The data* you need to complete this tutorial can be downloaded from <a href="https://github.com/ourcodingclub/CC-topic-modelling-python" target="_blank" rel="noopener noreferrer">this repository</a>. <strong>Click on <code class="language-plaintext highlighter-rouge">Clone/Download/Download ZIP</code> and unzip the folder, or clone the repository to your own GitHub account.</strong>
</h3>

<p>* The original dataset was taken from <a href="https://data.world/crowdflower/sentiment-of-climate-change" target="_blank" rel="noopener noreferrer">the data.world website</a> but we have modified it slightly, so for this tutorial you should use the version on our Github.</p>

<p>Import these packages next. You aren’t going to be able to complete this tutorial without them. You are also going to need the <code class="language-plaintext highlighter-rouge">nltk</code> package, which we will talk a little more about later in the tutorial.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># packages to store and manipulate data
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># plotting packages
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># model building package
</span><span class="kn">import</span> <span class="nn">sklearn</span>

<span class="c1"># package to clean text
</span><span class="kn">import</span> <span class="nn">re</span>
</code></pre></div></div>
<p>Next we will read in this dataset and have a look at it. You should use the <code class="language-plaintext highlighter-rouge">read_csv</code> function from <code class="language-plaintext highlighter-rouge">pandas</code> to read it in.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'climate_tweets.csv'</span><span class="p">)</span>
</code></pre></div></div>

<p>Have a quick look at your dataframe, it should look like this:</p>

<center>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tweet</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. [link]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Fighting poverty and global warming in Africa [link]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Carbon offsets: How a Vatican forest failed to reduce global warming [link]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Carbon offsets: How a Vatican forest failed to reduce global warming [link]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>URUGUAY: Tools Needed for Those Most Vulnerable to Climate Change [link]</td>
    </tr>
  </tbody>
</table>
</center>

<p>Note that some of the web links have been replaced by [link], but some have not. This was in the dataset when we downloaded it initially and it will be in yours. This doesn’t matter for this tutorial, but it always good to question what has been done to your dataset before you start working with it.</p>

<p><a name="eda"></a></p>
<h1 id="eda---time-to-start-exploring-our-dataset">EDA - Time to start exploring our dataset</h1>

<p>Find out the shape of your dataset to find out how many tweets we have. You can use <code class="language-plaintext highlighter-rouge">df.shape</code> where <code class="language-plaintext highlighter-rouge">df</code> is your dataframe.</p>

<p>One thing we should think about is how many of our tweets are actually unique because people retweet each other and so there could be multiple copies of the same tweet. You can do this using the <code class="language-plaintext highlighter-rouge">df.tweet.unique().shape</code>.</p>

<p>You may have seen when looking at the dataframe that there were tweets that started with the letters ‘RT’. Unsurprisingly this is a ReTweet. In the line below we will find how many of the of the tweets start with ‘RT’ and hence how many of them are retweets. We will be doing this with the pandas series <code class="language-plaintext highlighter-rouge">.apply</code> method. You can use the <code class="language-plaintext highlighter-rouge">.apply</code> method to apply a function to the values in each cell of a column.</p>

<p>We are going to be using <strong>lambda functions</strong> and <strong>string comparisons</strong> to find the retweets. If you don’t know what these two methods then read on for the basics.</p>

<h3 id="string-comparisons">String Comparisons</h3>

<p>String comparisons in Python are pretty simple. Like any comparison we use the <code class="language-plaintext highlighter-rouge">==</code> operator in order to see if two strings are the same. For example if</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># two string variables for comparison
</span><span class="n">string1</span> <span class="o">=</span> <span class="s">'climate'</span>
<span class="n">string2</span> <span class="o">=</span> <span class="s">'climb'</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">string1 == string2</code> will evaluate to <code class="language-plaintext highlighter-rouge">False</code>.</p>

<p>We can also slice strings to compare their parts, for example <code class="language-plaintext highlighter-rouge">string1[:4] == string2[:4]</code> will evaluate to <code class="language-plaintext highlighter-rouge">True</code>.</p>

<p>We are going to use this kind of comparison to see if each tweet beings with ‘RT’. If this evaluates to <code class="language-plaintext highlighter-rouge">True</code> then we will know it is a retweet.</p>

<h3 id="lambda-functions">Lambda Functions</h3>

<p>Lambda functions are a quick (and rather dirty) way of writing functions. The format of writing these functions is
<code class="language-plaintext highlighter-rouge">my_lambda_function = lambda x: f(x)</code> where we would replace <code class="language-plaintext highlighter-rouge">f(x)</code> with any function like <code class="language-plaintext highlighter-rouge">x**2</code> or <code class="language-plaintext highlighter-rouge">x[:2] + ' are the first to characters'</code>.</p>

<p>Here is an example of the same function written in the more formal method and with a lambda function</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># normal function example
</span><span class="k">def</span> <span class="nf">my_normal_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span>
<span class="c1"># lambda function example
</span><span class="n">my_lambda_function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span>
</code></pre></div></div>

<p>Try copying the functions above and seeing that they give the same results for the same inputs.</p>

<h3 id="finding-retweets">Finding Retweets</h3>

<p>Now that we have briefly covered string comparisons and lambda functions we will use these to find the number of retweets. Use the lines below to find out how many retweets there are in the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make a new column to highlight retweets
</span><span class="n">df</span><span class="p">[</span><span class="s">'is_retweet'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'tweet'</span><span class="p">]</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s">'RT'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'is_retweet'</span><span class="p">]</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>  <span class="c1"># number of retweets
</span></code></pre></div></div>

<p>You can also use the line below to find out the number of unique retweets</p>

<pre><code class="language-Python"> # number of unique retweets
df.loc[df['is_retweet']].tweet.unique().size
</code></pre>
<p>Next we would like to see the popular tweets. We will count the number of times that each tweet is repeated in our dataframe, and sort by the number of times that each tweet appears. Then we will look at the top 10 tweets. You can do this by printing the following manipulation of our dataframe:</p>
<pre><code class="language-Python"># 10 most repeated tweets
df.groupby(['tweet']).size().reset_index(name='counts')\
  .sort_values('counts', ascending=False).head(10)
</code></pre>
<p>One of the top tweets will be this one</p>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tweet</th>
      <th>counts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4555</th>
      <td>Take Action @change: Help Protect Wildlife Habitat from Climate Change [link]</td>
      <td>14</td>
    </tr>
  </tbody>
</table>

<p>It is informative to see the top 10 tweets, but it may also be informative to see how the number-of-copies of each tweet are distributed. We do that with the following code block.</p>
<pre><code class="language-Python"># number of times each tweet appears
counts = df.groupby(['tweet']).size()\
           .reset_index(name='counts')\
           .counts

# define bins for histogram
my_bins = np.arange(0,counts.max()+2, 1)-0.5

# plot histogram of tweet counts
plt.figure()
plt.hist(counts, bins = my_bins)
plt.xlabels = np.arange(1,counts.max()+1, 1)
plt.xlabel('copies of each tweet')
plt.ylabel('frequency')
plt.yscale('log', nonposy='clip')
plt.show()
</code></pre>

<center><img src="/img/topic-modelling-python-tweet_distribution.png" alt="img" style="width: 900px;"></center>

<p><a name="who_what"></a></p>
<h1 id="who-what---extracting-substrings-with-regular-expressions">@who? #what? - Extracting substrings with regular expressions</h1>

<p>Next lets find who is being tweeting at the most, retweeted the most, and what are the most common hashtags.</p>

<p>In the following section I am going to be using the python <code class="language-plaintext highlighter-rouge">re</code> package (which stands for Regular Expression), which an important package for text manipulation and complex enough to be the subject of its own tutorial. I am therefore going to skim over the details of this package and just leave you with some working code.</p>

<p>If you would like to know more about the <code class="language-plaintext highlighter-rouge">re</code> package and regular expressions you can find a good tutorial <a href="https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial" target="_blank" rel="noopener noreferrer">here on datacamp</a>.</p>

<p>As a quick overview the <code class="language-plaintext highlighter-rouge">re</code> package can be used to extract or replace certain patterns in string data in Python. You can use this package for anything from removing sensitive information like dates of birth and account numbers, to extracting all sentences that end in a :), to see what is making people happy.</p>

<p>In this tutorial we are going to be using this package to extract from each tweet:</p>
<ul>
  <li>who is being retweeted (if any)</li>
  <li>who is being tweeted at/mentioned (if any)</li>
  <li>what hashtags are being used (if any)</li>
</ul>

<p>Functions to extract each of these three things are below.</p>

<pre><code class="language-Python">def find_retweeted(tweet):
    '''This function will extract the twitter handles of retweed people'''
    return re.findall('(?&lt;=RT\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)

def find_mentioned(tweet):
    '''This function will extract the twitter handles of people mentioned in the tweet'''
    return re.findall('(?&lt;!RT\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)  

def find_hashtags(tweet):
    '''This function will extract hashtags'''
    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)   
</code></pre>
<p>Try using each of the functions above on the following tweets</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># two sample tweets
</span><span class="n">my_tweet</span> <span class="o">=</span> <span class="s">'RT @our_codingclub: Can @you find #all the #hashtags?'</span><span class="err">`</span>
<span class="n">my_other_tweet</span> <span class="o">=</span> <span class="s">'Not a retweet. All views @my own'</span>
</code></pre></div></div>

<p>We are now going to make one column in the dataframe which contains the retweet handles, one column for the handles of people mentioned and one columns for the hashtags. We will do this by using the <code class="language-plaintext highlighter-rouge">.apply</code> method three times.</p>

<p>Note that each entry in these new columns will contain a list rather than a single value</p>

<pre><code class="language-Python"># make new columns for retweeted usernames, mentioned usernames and hashtags
df['retweeted'] = df.tweet.apply(find_retweeted)
df['mentioned'] = df.tweet.apply(find_mentioned)
df['hashtags'] = df.tweet.apply(find_hashtags)
</code></pre>
<p>Print the dataframe again to have a look at the new columns. Your dataframe should now look like this:</p>

<center>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tweet</th>
      <th>retweeted</th>
      <th>mentioned</th>
      <th>hashtags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>36</th>
      <td>RT @virgiltexas: Hey Al Gore: see these tornadoes racing across Mississippi? So much for global "warming" #tornadocot #ocra #sgp #gop #ucot #tlot #p2 #tycot</td>
      <td>[@virgiltexas]</td>
      <td>[]</td>
      <td>[#tornadocot, #ocra, #sgp, #gop, #ucot, #tlot, #p2, #tycot]</td>
    </tr>
    <tr>
      <th>37</th>
      <td>#justinbiebersucks and global warming is a farce</td>
      <td>[]</td>
      <td>[]</td>
      <td>[#justinbiebersucks]</td>
    </tr>
    <tr>
       <th>297</th>
       <td>Just briefed on global cooling &amp; volcanoes via @abc But I wonder ... if it gets to the stratosphere can it slow/improve global warming??</td>
       <td>[]</td>
       <td>[@abc]</td>
       <td>[]</td>
     </tr>
     <tr>
       <th>298</th>
       <td>Climate Change-ing your Allergies [link]</td>
       <td>[]</td>
       <td>[]</td>
       <td>[]</td>
     </tr>
  </tbody>
</table>
</center>

<p><a name="text_corr"></a></p>
<h1 id="keyword-correlations-in-text">Keyword Correlations in Text</h1>

<p>So far we have extracted who was retweeted, who was mentioned and the hashtags into their own separate columns. Now lets look at these further. We want to know who is highly retweeted, who is highly mentioned and what popular hashtags are going round.</p>

<p>In the following section we will perform an analysis on the hashtags only. We will leave it up to you to come back and repeat a similar analysis on the mentioned and retweeted columns.</p>

<p>First we will select the column of hashtags from the dataframe, and take only the rows where there actually is a hashtag</p>

<pre><code class="language-Python"># take the rows from the hashtag columns where there are actually hashtags
hashtags_list_df = df.loc[
                       df.hashtags.apply(
                           lambda hashtags_list: hashtags_list !=[]
                       ),['hashtags']]
</code></pre>

<center>
The first few rows of `hashtags_list_df` should look like this
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hashtags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>[#Climate, #population]</td>
    </tr>
    <tr>
      <th>16</th>
      <td>[#EarthDay]</td>
    </tr>
    <tr>
      <th>26</th>
      <td>[#ac]</td>
    </tr>
    <tr>
      <th>31</th>
      <td>[#tcot]</td>
    </tr>
    <tr>
      <th>36</th>
      <td>[#tornadocot, #ocra, #sgp, #gop, #ucot, #tlot, #p2, #tycot]</td>
    </tr>
  </tbody>
</table>
</center>

<p>To see which hashtags were popular we will need to flatten out this dataframe. Currently each row contains a list of multiple values. The next block of code will make a new dataframe where we take all the hashtags in <code class="language-plaintext highlighter-rouge">hashtags_list_df</code> but give each its own row.</p>

<p>We do this using a <a href="https://www.pythonforbeginners.com/basics/list-comprehensions-in-python" target="_blank" rel="noopener noreferrer">list comprehension</a>.</p>

<pre><code class="language-Python"># create dataframe where each use of hashtag gets its own row
flattened_hashtags_df = pd.DataFrame(
    [hashtag for hashtags_list in hashtags_list_df.hashtags
    for hashtag in hashtags_list],
    columns=['hashtag'])
</code></pre>
<center>
This new dataframe will look like this
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hashtag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>#Climate</td>
    </tr>
    <tr>
      <th>1</th>
      <td>#population</td>
    </tr>
    <tr>
      <th>2</th>
      <td>#EarthDay</td>
    </tr>
  </tbody>
</table>
</center>

<p>Now, as we did with the full tweets before, you should find the number of unique rows in this dataframe. Before this was the unique number of tweets, now the unique number of hashtags.</p>

<pre><code class="language-Python"># number of unique hashtags
flattened_hashtags_df['hashtag'].unique().size
</code></pre>
<p>Like before lets look at the top hashtags by their frequency of appearance. You can do this using</p>
<pre><code class="language-Python"># count of appearances of each hashtag
popular_hashtags = flattened_hashtags_df.groupby('hashtag').size()\
                                        .reset_index(name='counts')\
                                        .sort_values('counts', ascending=False)\
                                        .reset_index(drop=True)
</code></pre>

<p>A big part of data science is in interpreting our results. Therefore domain knowledge needs to be incorporated to get the best out of the analysis we do. Sometimes this can be as simple as a Google search so lets do that here.</p>

<p>If you do not know what the top hashtag means, try googling it. Does it make sense for this to be the top hashtag in the context of tweets about climate change? Was this top hashtag big at a particular point in time and do you think it would still be the top hashtag today?</p>

<p>Once you have done that,  plot the distribution in how often these hashtags appear</p>
<pre><code class="language-Python"># number of times each hashtag appears
counts = flattened_hashtags_df.groupby(['hashtag']).size()\
                              .reset_index(name='counts')\
                              .counts

# define bins for histogram                              
my_bins = np.arange(0,counts.max()+2, 5)-0.5

# plot histogram of tweet counts
plt.figure()
plt.hist(counts, bins = my_bins)
plt.xlabels = np.arange(1,counts.max()+1, 1)
plt.xlabel('hashtag number of appearances')
plt.ylabel('frequency')
plt.yscale('log', nonposy='clip')
plt.show()
</code></pre>
<p><strong>When you finish this section you could repeat a similar process to find who were the top people that were being retweeted and who were the top people being mentioned</strong></p>

<h3 id="from-text-to-vector">From Text to Vector</h3>

<p>Now lets say that we want to find which of our hashtags are correlated with each other. To do this we will need to turn the text into numeric form. It is possible to do this by transforming from a list of hashtags to a vector representing which hashtags appeared in which rows. For example if our available hashtags were the set <code class="language-plaintext highlighter-rouge">[#photography, #pets, #funny, #day]</code>, then the tweet ‘#funny #pets’ would be <code class="language-plaintext highlighter-rouge">[0,1,1,0]</code> in vector form.</p>

<p>We will now apply this method to our hashtags column of <code class="language-plaintext highlighter-rouge">df</code>. Before we do this we will want to limit to hashtags that appear enough times to be correlated with other hashtags. We can’t correlate hashtags which only appear once, and we don’t want hashtags that appear a low number of times since this could lead to spurious correlations.</p>

<p>In the following code block we are going to find what hashtags meet a minimum appearance threshold. These are going to be the hashtags we will look for correlations between.</p>

<pre><code class="language-Python"># take hashtags which appear at least this amount of times
min_appearance = 10
# find popular hashtags - make into python set for efficiency
popular_hashtags_set = set(popular_hashtags[
                           popular_hashtags.counts&gt;=min_appearance
                           ]['hashtag'])
</code></pre>
<p>Next we are going to create a new column in <code class="language-plaintext highlighter-rouge">hashtags_df</code> which filters the hashtags to only the popular hashtags. We will also drop the rows where no popular hashtags appear.</p>

<pre><code class="language-Python"># make a new column with only the popular hashtags
hashtags_list_df['popular_hashtags'] = hashtags_list_df.hashtags.apply(
            lambda hashtag_list: [hashtag for hashtag in hashtag_list
                                  if hashtag in popular_hashtags_set])
# drop rows without popular hashtag
popular_hashtags_list_df = hashtags_list_df.loc[
            hashtags_list_df.popular_hashtags.apply(lambda hashtag_list: hashtag_list !=[])]

</code></pre>
<p>Next we want to vectorise our the hashtags in each tweet like mentioned above. We do this using the following block of code to create a dataframe where the hashtags contained in each row are in vector form.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make new dataframe
</span><span class="n">hashtag_vector_df</span> <span class="o">=</span> <span class="n">popular_hashtags_list_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s">'popular_hashtags'</span><span class="p">]]</span>

<span class="k">for</span> <span class="n">hashtag</span> <span class="ow">in</span> <span class="n">popular_hashtags_set</span><span class="p">:</span>
    <span class="c1"># make columns to encode presence of hashtags
</span>    <span class="n">hashtag_vector_df</span><span class="p">[</span><span class="s">'{}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">hashtag</span><span class="p">)]</span> <span class="o">=</span> <span class="n">hashtag_vector_df</span><span class="o">.</span><span class="n">popular_hashtags</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">hashtag_list</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">hashtag</span> <span class="ow">in</span> <span class="n">hashtag_list</span><span class="p">))</span>
</code></pre></div></div>
<p>Print the <code class="language-plaintext highlighter-rouge">hashtag_vector_df</code> to see that the vectorisation has gone as expected. For each hashtag in the <code class="language-plaintext highlighter-rouge">popular_hashtags</code> column there should be a 1 in the corresponding <code class="language-plaintext highlighter-rouge">#hashtag</code> column. It should look something like this:</p>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>popular_hashtags</th>
      <th>#environment</th>
      <th>#EarthDay</th>
      <th>#gop</th>
      <th>#snowpocalypse</th>
      <th>#Climate</th>
      <th>...</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>[#Climate]</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
    </tr>
    <tr>
      <th>16</th>
      <td>[#EarthDay]</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
    </tr>
  </tbody>
</table>

<p>Now satisfied we will drop the <code class="language-plaintext highlighter-rouge">popular_hashtags</code> column from the dataframe. We don’t need it.</p>

<p><code class="language-plaintext highlighter-rouge">hashtag_matrix = hashtag_vector_df.drop('popular_hashtags', axis=1)</code>`</p>

<p>In the next code block we will use the <code class="language-plaintext highlighter-rouge">pandas.DataFrame</code> inbuilt method to find the correlation between each column of the dataframe and thus the correlation between the different hashtags appearing in the same tweets.</p>

<p>We will use the <code class="language-plaintext highlighter-rouge">seaborn</code> package that we imported earlier to plot the correlation matrix as a heatmap</p>
<pre><code class="language-Python"># calculate the correlation matrix
correlations = hashtag_matrix.corr()

# plot the correlation matrix
plt.figure(figsize=(10,10))
sns.heatmap(corrleations,
    cmap='RdBu',
    vmin=-1,
    vmax=1,
    square = True,
    cbar_kws={'label':'correlation'})
plt.show()
</code></pre>
<center><img src="/img/topic-modelling-python-hashtag_correlation.png" alt="img" style="width: 900px;"></center>

<p>From the plot above we can see that there are fairly strong correlations between</p>
<ul>
  <li>
<strong>#SaveTerra</strong> and <strong>#SierraClub</strong>
</li>
  <li>
<strong>#GloablWarming</strong> and <strong>#FoxNews</strong>
</li>
</ul>

<p>We can also see a fairly strong negative correlation between</p>
<ul>
  <li>
<strong>#tcot</strong> and <strong>#climate</strong>
</li>
</ul>

<p>What these really mean is up for interpretation and it won’t be the focus of this tutorial.</p>

<p><a name="top_mod"></a></p>
<h1 id="introduction-to-topic-modelling">Introduction to Topic Modelling</h1>

<p>What we have done so far with the hashtags has given us a bit more of an insight into the kind of things that people are tweeting about. We used our correlations to better understand the hashtag topics in the dataset (a kind of dimensionality reduction by looking only at the highly correlated ones). The correlation between <strong>#FoxNews</strong> and <strong>#GlobalWarming</strong> gives us more information as a pair than they do separately.</p>

<p>But what about all the other text in the tweet besides the #hashtags and @users? Surely there is lots of useful and meaningful information in there as well? Yes! Absolutely, but we can’t just do correlations like we have done here. There are far too many different words for that! We need a new technique!</p>

<p>…enter topic modelling</p>

<p>Topic modelling is an unsupervised machine learning algorithm for discovering ‘topics’ in a collection of documents. In this case our collection of documents is actually a collection of tweets. We won’t get too much into the details of the algorithms that we are going to look at since they are complex and beyond the scope of this tutorial. We will be using latent dirichlet allocation (LDA) and at the end of this tutorial we will leave you to implement non-negative matric factorisation (NMF) by yourself.</p>

<p>The important information to know is that these techniques each take a matrix which is similar to the <code class="language-plaintext highlighter-rouge">hashtag_vector_df</code> dataframe that we created above. Every row represents a tweet and every column represents a word. The entry at each row-column position is the number of times that a given word appears in the tweet for the row, this is called the bag-of-words format. For the word-set <code class="language-plaintext highlighter-rouge">[#photography, #pets, #funny, #day]</code>, the tweet ‘#funny #funny #photography #pets’ would be <code class="language-plaintext highlighter-rouge">[1,1,2,0]</code> in vector form.</p>

<p>Using this matrix the topic modelling algorithms will form topics from the words. Each of the algorithms does this in a different way, but the basics are that the algorithms look at the co-occurrence of words in the tweets and if words often appearing in the same tweets together, then these words are likely to form a topic together. The algorithm will form topics which group commonly co-occurring words. A topic in this sense, is just list of words that often appear together and also scores associated with each of these words in the topic. The higher the score of a word in a topic, the higher that word’s importance in the topic. Each topic will have a score for every word found in tweets, in order to make sense of the topics we usually only look at the top words - the words with low scores are irrelevant.</p>

<p>For example, from a topic model built on a collection on marine research articles might find the topic</p>

<ul>
  <li>asteroidea, starfish, legs, regenerate, ecological, marine, asexually, …</li>
</ul>

<p>and the accompanying scores for each word in this topic could be</p>

<ul>
  <li>900, 666, 523, 503, 392, 299, 127, …</li>
</ul>

<p>We can see that this seems to be a general topic about starfish, but the important part is that <strong>we have to decide what these topics mean</strong> by interpreting the top words. The model will find us as many topics as we tell it to, this is an important choice to make. Too large and we will likely only find very general topics which don’t tell us anything new, too few and the algorithm way pick up on noise in the data and not return meaningful topics. So this is an important parameter to think about.</p>

<p>This has been a rapid introduction to topic modelling, in order to help our topic modelling algorithms along we will first need to clean up our data.</p>

<p><a name="clean"></a></p>
<h1 id="cleaning-unstructured-text-data">Cleaning Unstructured Text Data</h1>

<p>The most important thing we need to do to help our topic modelling algorithm is to pre-clean up the tweets. If you look back at the tweets you may notice that they are very untidy, with non-standard English, capitalisation, links, hashtags, @users and punctuation and emoticons everywhere. If we are going to be able to apply topic modelling we need to remove most of this and massage our data into a more standard form before finally turning it into vectors.</p>

<p>In this section I will provide some functions for cleaning the tweets as well as the reasons for each step in cleaning. I won’t cover the specifics of the package we are going to use. The use of the Python <code class="language-plaintext highlighter-rouge">nltk</code> package and how to properly and efficiently clean text data could be another full tutorial itself so I hope that this is enough just to get you started.</p>

<p>First we will start with imports for this specific cleaning task.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">RegexpTokenizer</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
</code></pre></div></div>

<p>You will need to use <code class="language-plaintext highlighter-rouge">nltk.download('stopwords')</code> command to download the stopwords if you have not used <code class="language-plaintext highlighter-rouge">nltk</code> before.</p>

<p>In the cell below I have provided you some functions to remove web-links from the tweets. I don’t think specific web links will be important information, although if you wanted to could replace all web links with a token (a word) like web_link, so you preserve the information that there was a web link there without preserving the link itself. In this case however, we will remove links. We will also remove retweets and mentions. We remove these because it is unlikely that they will help us form meaningful topics.</p>

<p><strong>We would like to know the general things which people are talking about, not who they are talking about or to and not the web links they are sharing.</strong></p>

<p><strong>Extra challenge:</strong> modify and use the <code class="language-plaintext highlighter-rouge">remove_links</code> function below in order to extract the links from each tweet to a separate column, then repeat the analysis we did on the hashtags. Are there any common links that people are sharing?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">remove_links</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
    <span class="s">'''Takes a string and removes web links from it'''</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r'http\S+'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span> <span class="c1"># remove http links
</span>    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r'bit.ly/\S+'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span> <span class="c1"># rempve bitly links
</span>    <span class="n">tweet</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s">'[link]'</span><span class="p">)</span> <span class="c1"># remove [links]
</span>    <span class="k">return</span> <span class="n">tweet</span>

<span class="k">def</span> <span class="nf">remove_users</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
    <span class="s">'''Takes a string and removes retweet and @user information'''</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'(RT</span><span class="err">\</span><span class="s">s@[A-Za-z]+[A-Za-z0-9-_]+)'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span> <span class="c1"># remove retweet
</span>    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'(@[A-Za-z]+[A-Za-z0-9-_]+)'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span> <span class="c1"># remove tweeted at
</span>    <span class="k">return</span> <span class="n">tweet</span>
</code></pre></div></div>
<p>Below we make a master function which uses the two functions we created above as sub functions. This is a common way of working in Python and makes your code tidier and more reusable. The master function will also do some more cleaning of the data.</p>

<p>This following section of bullet points describes what the <code class="language-plaintext highlighter-rouge">clean_tweet</code> master function is doing at each step. If you want you can skip reading this section and just use the function for now. You will likely notice some strange words in your topics later, so when you finally generate them you should come back to second last bullet point about <strong>stem</strong>ming.</p>

<p>In the master function we apply these steps in order:</p>
<ul>
  <li>
    <p>Strip out the users and links from the tweets but we leave the hashtags as I believe those can still tell us what people are talking about in a more general way.</p>
  </li>
  <li>
    <p>After this we make the whole tweet lowercase as otherwise the algorithm would think that the words ‘climate’ and ‘Climate’ were the same. ie it is case sensitive.</p>
  </li>
  <li>
    <p>Next we remove punctuation characters, contained in the <code class="language-plaintext highlighter-rouge">my_punctuation</code> string,  to further tidy up the text. We need to do this or we could find tokens* which have punctuation at the end or in the middle.</p>
  </li>
  <li>
    <p>In the next two steps we remove double spacing that may have been caused by the punctuation removal and remove numbers.</p>
  </li>
</ul>

<p><em>By now the data is a lot tidier and we have only lowercase letters which are space separated. The only punctuation is the ‘#’ in the hashtags.</em></p>
<ul>
  <li>
    <p>Next we change the form of our tweet from a string to a list of words. We also remove stopwords in this step. Stopwords are simple words that don’t tell us very much. Print the <code class="language-plaintext highlighter-rouge">my_stopwords</code> variable to see what words we are removing and think whether you can still get the gist of any sentence if you were to take out these words.</p>
  </li>
  <li>
    <p>In the next step we <strong>stem</strong> the words in the list. This is essentially where we knock the end off the words. We do this so that similar words will be recognised as the same word by the algorithm. For example in the starfish example we would like it so that the algorithm knows that when it sees ‘regenerate’, ‘regenerated’, ‘regenerates’, ‘regeneration’ or ‘regenerating’ that it will know these are really the same word whilst it is building up topics. It can’t do this itself, so we knock off the word endings so that each of these words will become the same stem - ‘regener’. Once you have copied the <code class="language-plaintext highlighter-rouge">word_rooter</code> function, use this line of code to see that these words all become the same thing <code class="language-plaintext highlighter-rouge">[word_rooter(w) for w in ['regenerate', 'regenerated', 'regenerates', 'regeneration', 'regenerating', 'regenerative']]</code>. Note that the <code class="language-plaintext highlighter-rouge">word_rooter</code> function, which is a Porter Stemming function, only uses rules of thumb to know where to cut off words, and so for the word ‘regenerative’ it will actually give it a different root to the other words.</p>
  </li>
  <li>
    <p>If we decide to use it the next step will construct bigrams from our tweet. This part of the function will group every pair of words and put them at the end. So the sentence <code class="language-plaintext highlighter-rouge">'i scream for ice cream'</code> becomes <code class="language-plaintext highlighter-rouge">'i scream for ice cream i_scream scream_for for_ice ice_cream'</code>. A bigram is a word pair like i_scream or ice_cream. The reason for doing this is that when we go from sentence to vector form of the tweets, we will lose the information about word ordering. Therefore we could lose ‘ice cream’ amongst tweets about putting ice and antiseptic cream on a wound (for example). Later we will filter by appearance frequency and so unnatural bigrams like ‘for_ice’ will be thrown out as they won’t appear enough to make it into the most popular tokens*.</p>
  </li>
</ul>

<p>* In natural language processing people talk about tokens instead of words but they basically mean the same thing. Here we have 3 kinds of tokens which make it through our cleaning process. We have words, bigrams and #hashtags.</p>

<p>In the next code block we make a function to clean the tweets.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_stopwords</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">)</span>
<span class="n">word_rooter</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">snowball</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">(</span><span class="n">ignore_stopwords</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">stem</span>
<span class="n">my_punctuation</span> <span class="o">=</span> <span class="s">'!"$</span><span class="si">%</span><span class="s">&amp;</span><span class="se">\'</span><span class="s">()*+,-./:;&lt;=&gt;?[</span><span class="se">\\</span><span class="s">]^_`{|}~•@'</span>

<span class="c1"># cleaning master function
</span><span class="k">def</span> <span class="nf">clean_tweet</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">bigrams</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">remove_users</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">remove_links</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="c1"># lower case
</span>    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'['</span><span class="o">+</span><span class="n">my_punctuation</span> <span class="o">+</span> <span class="s">']+'</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span> <span class="c1"># strip punctuation
</span>    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'</span><span class="err">\</span><span class="s">s+'</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span> <span class="c1">#remove double spacing
</span>    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'([0-9]+)'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span> <span class="c1"># remove numbers
</span>    <span class="n">tweet_token_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tweet</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">my_stopwords</span><span class="p">]</span> <span class="c1"># remove stopwords
</span>
    <span class="n">tweet_token_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_rooter</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">if</span> <span class="s">'#'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word</span> <span class="k">else</span> <span class="n">word</span>
                        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tweet_token_list</span><span class="p">]</span> <span class="c1"># apply word rooter
</span>    <span class="k">if</span> <span class="n">bigrams</span><span class="p">:</span>
        <span class="n">tweet_token_list</span> <span class="o">=</span> <span class="n">tweet_token_list</span><span class="o">+</span><span class="p">[</span><span class="n">tweet_token_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="s">'_'</span><span class="o">+</span><span class="n">tweet_token_list</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tweet_token_list</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweet_token_list</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tweet</span>
</code></pre></div></div>

<p>Use the cleaning function above to make a new column of cleaned tweets. Set <code class="language-plaintext highlighter-rouge">bigrams = False</code> for the moment to keep things simple. This is something you could come back to later. Print this new column see if you can understand the gist of what each tweet is about.</p>

<pre><code class="language-Python">df['clean_tweet'] = df.tweet.apply(clean_tweet)
</code></pre>

<center>Your new dataframe should look something like this
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tweet</th>
      <th>is_retweet</th>
      <th>retweeted</th>
      <th>mentioned</th>
      <th>hashtags</th>
      <th>clean_tweet</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>Carbon offsets: How a Vatican forest failed to reduce global warming [link]</td>
      <td>False</td>
      <td>[]</td>
      <td>[]</td>
      <td>[]</td>
      <td>carbon offset vatican forest fail reduc global warm</td>
    </tr>
    <tr>
      <th>4</th>
      <td>URUGUAY: Tools Needed for Those Most Vulnerable to Climate Change [link]</td>
      <td>False</td>
      <td>[]</td>
      <td>[]</td>
      <td>[]</td>
      <td>uruguay tool need vulner climat chang</td>
    </tr>
    <tr>
      <th>5</th>
      <td>RT @sejorg: RT @JaymiHeimbuch: Ocean Saltiness Shows Global Warming Is Intensifying Our Water Cycle [link]</td>
      <td>True</td>
      <td>[@sejorg, @JaymiHeimbuch]</td>
      <td>[]</td>
      <td>[]</td>
      <td>ocean salti show global warm intensifi water cycl</td>
    </tr>
  </tbody>
</table>
</center>

<p><a name="apply"></a></p>
<h1 id="applying-topic-modelling">Applying Topic Modelling</h1>

<p>Good news! We are almost there! Now that we have clean text we can use some standard Python tools to turn the text tweets into vectors and then build a model.</p>

<p>To turn the text into a matrix*, where each row in the matrix encodes which words appeared in each individual tweet. We will also filter the words <code class="language-plaintext highlighter-rouge">max_df=0.9</code> means we discard any words that appear in &gt;90% of tweets. In this dataset I don’t think there are any words that are that common but it is good practice. We will also filter words using <code class="language-plaintext highlighter-rouge">min_df=25</code>, so words that appear in less than 25 tweets will be discarded. We discard high appearing words since they are too common to be meaningful in topics. We discard low appearing words because we won’t have a strong enough signal and they will just introduce noise to our model.</p>

<p>* We usually turn text into a sparse matrix, to save on space, but since our tweet database it small we should be able to use a normal matrix.</p>

<pre><code class="language-Python">from sklearn.feature_extraction.text import CountVectorizer

# the vectorizer object will be used to transform text to vector form
vectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\w+|\$[\d\.]+|\S+')

# apply transformation
tf = vectorizer.fit_transform(df['clean_tweet']).toarray()

# tf_feature_names tells us what word each column in the matric represents
tf_feature_names = vectorizer.get_feature_names()
</code></pre>
<p>Check out the shape of <code class="language-plaintext highlighter-rouge">tf</code> (we chose tf as a variable name to stand for ‘term frequency’ - the frequency of each word/token in each tweet). The shape of tf tells us how many tweets we have and how many words we have that made it through our filtering process.</p>

<p>Whilst you are here, you should also print <code class="language-plaintext highlighter-rouge">tf_feature_names</code> to see what tokens made it through filtering.</p>

<p>Note that the <code class="language-plaintext highlighter-rouge">tf</code> matrix is exactly like the <code class="language-plaintext highlighter-rouge">hashtag_vector_df</code> dataframe. Each row is a tweet and each column is a word. The numbers in each position tell us how many times this word appears in this tweet.</p>

<p>Next we actually create the model object. Lets start by arbitrarily choosing 10 topics. We also define the random state so that this model is reproducible.</p>

<pre><code class="language-Python">from sklearn.decomposition import LatentDirichletAllocation

number_of_topics = 10

model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)
</code></pre>
<p><code class="language-plaintext highlighter-rouge">model</code> is our LDA algorithm model object. I expect that if you are here then you should be comfortable with Python’s object orientation. If not then all you need to know is that the model object hold everything we need. It holds parameters like the number of topics that we gave it when we created it; it also holds methods like the fitting method; once we fit it, it will hold fitted parameters which tell us how important different words are in different topics. We will apply this next and feed it our <code class="language-plaintext highlighter-rouge">tf</code> matrix</p>
<pre><code class="language-Python">model.fit(tf)
</code></pre>
<p><strong>Congratulations!</strong> You have now fitted a topic model to tweets!</p>

<p>Next we will want to inspect our topics that we generated and try to extract meaningful information from them.</p>

<p>Below I have written a function which takes in our model object <code class="language-plaintext highlighter-rouge">model</code>, the order of the words in our matrix <code class="language-plaintext highlighter-rouge">tf_feature_names</code> and the number of words we would like to show. Use this function, which returns a dataframe, to show you the topics we created. Remember that each topic is a list of words/tokens and weights</p>

<pre><code class="language-Python">def display_topics(model, feature_names, no_top_words):
    topic_dict = {}
    for topic_idx, topic in enumerate(model.components_):
        topic_dict["Topic %d words" % (topic_idx)]= ['{}'.format(feature_names[i])
                        for i in topic.argsort()[:-no_top_words - 1:-1]]
        topic_dict["Topic %d weights" % (topic_idx)]= ['{:.1f}'.format(topic[i])
                        for i in topic.argsort()[:-no_top_words - 1:-1]]
    return pd.DataFrame(topic_dict)
</code></pre>
<p>You can apply this function like so</p>
<pre><code class="language-Python">no_top_words = 10
display_topics(model, tf_feature_names, no_top_words)
</code></pre>

<p>Now we have some topics, which are just clusters of words, we can try to figure out what they really mean. Once again, this is a task of interpretation, and so I will leave this task to you.</p>

<center>Here is an example of a few topics I got from my model. <br>Note that your topics will not necessarily include these three.<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Topic 3 words</th>
      <th>Topic 3 weights</th>
      <th>Topic 4 words</th>
      <th>Topic 4 weights</th>
      <th>Topic 5 words</th>
      <th>Topic 5 weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>global</td>
      <td>473.1</td>
      <td>climat</td>
      <td>422.0</td>
      <td>global</td>
      <td>783.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>warm</td>
      <td>450.7</td>
      <td>chang</td>
      <td>401.8</td>
      <td>warm</td>
      <td>764.7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>believ</td>
      <td>101.3</td>
      <td>legisl</td>
      <td>123.2</td>
      <td>gore</td>
      <td>137.1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>california</td>
      <td>87.1</td>
      <td>us</td>
      <td>105.1</td>
      <td>snow</td>
      <td>123.7</td>
    </tr>
    <tr>
      <th>4</th>
      <td>blame</td>
      <td>82.1</td>
      <td>via</td>
      <td>60.5</td>
      <td>al</td>
      <td>122.1</td>
    </tr>
  </tbody>
</table>
</center>

<h4 id="comment">Comment</h4>

<p>I found that my topics almost all had global warming or climate change at the top of the list. This could indicate that we should add these words to our stopwords like since they don’t tell us anything we didn’t already know. We already knew that the dataset was tweets about climate change.</p>

<p>This result also may have come from the fact that tweets are very short and this particular method, LDA (which works very well for longer text documents), does not work well on shorter text documents like tweets. In the bonus section to follow I suggest replacing the LDA model with an NMF model and try creating a new set of topics. In my own experiments I found that NMF generated better topics from the tweets than LDA did, even without removing ‘climate change’ and ‘global warming’ from the tweets.</p>

<p><a name="bonus"></a></p>
<h1 id="bonus">Bonus</h1>

<p>If you want to try out a different model you could use non-negative matrix factorisation (NMF). The work flow for this model will be almost exactly the same as with the LDA model we have just used, and the functions which we developed to plot the results will be the same as well. You can import the NMF model class by using <code class="language-plaintext highlighter-rouge">from sklearn.decomposition import NMF</code>.</p>

<ul>
  <li>
    <p>Building models on tweets is a particularly hard task for topic models since tweets are very short. Using <code class="language-plaintext highlighter-rouge">len(tweet_string.split(' '))</code> inside a lambda function and feeding this into a <code class="language-plaintext highlighter-rouge">.apply</code>, find out the mean value and distribution of how many words there are in each tweet after cleaning?</p>
  </li>
  <li>
    <p>Try to build an NMF model on the same data and see if the topics are the same? Different models have different strengths and so you may find NMF to be better. You can use <code class="language-plaintext highlighter-rouge">model = NMF(n_components=no_topics, random_state=0, alpha=.1, l1_ratio=.5)</code> and continue from there in your original script.</p>
  </li>
</ul>

<h3 id="further-extension">Further Extension</h3>

<ul>
  <li>
    <p>If you would like to do more topic modelling on tweets I would recommend the <code class="language-plaintext highlighter-rouge">tweepy</code> package. This is a Python package that allows you to download tweets from twitter. You have many options of which tweets to download including filtering to a particular area and to a particular time.</p>
  </li>
  <li>
    <p>Each of the topic models has its own set of parameters that you can change to try and achieve a better set of topics. Go to the sklearn site for the LDA and NMF models to see what these parameters and then try changing them to see how the affects your results.</p>
  </li>
</ul>

<h1 id="summary">Summary</h1>

<p>Topic modelling is a really useful tool to explore text data and find the latent topics contained within it. We have seen how we can apply topic modelling to untidy tweets by cleaning them first.</p>

<h3 id="tutorial-outcomes">Tutorial outcomes:</h3>

<ul>
  <li>
    <p>You have learned how to explore text datasets by extracting keywords and finding correlations</p>
  </li>
  <li>
    <p>You have been introduced to the <code class="language-plaintext highlighter-rouge">re</code> package and seen how it can be used to manipulate and clean text data</p>
  </li>
  <li>
    <p>You have been introduced to topic modelling and the LDA algorithm</p>
  </li>
  <li>
    <p>You have built you first topic model and visualised the results</p>
  </li>
</ul>

<hr>

<hr>

<h3><a href="https://www.surveymonkey.co.uk/r/7C5N3QV" target="_blank" rel="noopener noreferrer">  We would love to hear your feedback, please fill out our survey!</a></h3>
<p><br></p>
<h3>  You can contact us with any questions on <a href="mailto:ourcodingclub@gmail.com?Subject=Tutorial%20question" target="_top">ourcodingclub@gmail.com</a>
</h3>
<p><br></p>
<h3>  Related tutorials:</h3>

<p><br></p>
<h3>  Subscribe to our mailing list:</h3>
<div class="container">
	<div class="block">
        <!-- subscribe form start -->
		<div class="form-group">
			<form action="https://getsimpleform.com/messages?form_api_token=de1ba2f2f947822946fb6e835437ec78" method="post">
			<div class="form-group">
				<input type="text" class="form-control" name="Email" placeholder="Email" required="">
			</div>
			<div>
                        	<button class="btn btn-default" type="submit">Subscribe</button>
                    	</div>
                	</form>
		</div>
	</div>
</div>

<ul class="social-icons">
	<li>
		<h3>
			<a href="https://twitter.com/our_codingclub" target="_blank" rel="noopener noreferrer"> Follow our coding adventures on Twitter! <i class="fa fa-twitter"></i></a>
		</h3>
	</li>
</ul>

    	<footer class="footer">
	<hr>
	<div class="footer-container">
    	<ul class="footer-link-list">
        	<li><a href="/tutorials">Tutorials</a></li>
        	<li><a href="/team">About Us</a></li>
        	<li><a href="/contact">Contact us</a></li>
	    	<li><a href="https://twitter.com/our_codingclub" target="_blank" rel="noopener noreferrer">Follow us on Twitter</a></li>
    	</ul>
    	<div class="footer-text">
			<p>We are happy for people to use and further develop our tutorials - please give credit to Coding Club by linking to <a href="https://ourcodingclub.github.io/" target="_blank" rel="noopener noreferrer">our website</a>. We are also happy to discuss possible collaborations, so get in touch at <b>ourcodingclub@gmail.com</b></p>
    		<p>See our <a href="/terms">Terms of Use</a> and our <a href="/privacy">Data Privacy policy</a>.</p>
			<p>This work is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener noreferrer">Creative Commons Attribution-ShareAlike 4.0 International License</a></p>
			<a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener noreferrer"><img class="license" src="https://licensebuttons.net/l/by-sa/4.0/80x15.png" alt="CC-by-sa-4.0"></a>
    	</div>
    </div>
</footer>

<!-- JS -->
<script src="/scripts/owl.carousel.js"></script>
<script src="/scripts/owl.carousel-init.js"></script>
<script src="/scripts/reveal.js"></script>


	
	</body>
</html>
