<!DOCTYPE html>
<html lang="en-GB">
	<head>
    	<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Intro to Machine Learning in R (K Nearest Neighbours Algorithm)</title>
<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- CSS -->
<link rel="stylesheet" href="/css/main.css"/>
<link rel="stylesheet" href="/css/owlcarousel/owl.carousel.css">
<link rel="stylesheet" href="/css/owlcarousel/owl.theme.default.css">

<!-- JS -->
<script src="https://use.fontawesome.com/4dd22df1f8.js"></script>
<script src="https://code.jquery.com/jquery-1.12.4.js"></script>
<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
<script src="/scripts/accordion.js"></script>
<script src="/scripts/jquery.counterup.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/waypoints/2.0.5/waypoints.min.js"></script>
<script src="/scripts/ticker.js"></script>

	</head>
	<body>
    	<header class="header">
	<div class="navigation-bar">
		<div id="navigation-container">
			
				<a class="logo" href="/">
  <img src="/assets/img/logos/logo_stack.svg" alt="Coding Club logo">
</a>

			
			<nav>
				<label for="hamburger">☰</label>
				<input type="checkbox" id="hamburger">
				<ul>
					
						
					<li class="item item-nav">
						<a href="/">Home</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/tutorials.html">Tutorials</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/course.html">Course</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/team.html">Team</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/involve.html">Get involved</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/links.html">Links</a>
					</li>
					
						
					<li class="item item-nav">
						<a href="/contact.html">Contact</a>
					</li>
					
				</ul>
			</nav>
		</div>
	</div>
</header>

			<div class="block">
	<center>
		<img src="/img/tutheader_knn.png" alt="Img">
	</center>
</div>

<h3 id="tutorial-aims">Tutorial Aims:</h3>

<h4 id="-1-what-is-about-machine-learning"><a href="#intro"> 1. What is about machine learning</a></h4>

<h4 id="-2-train-your-algorithm"><a href="#train"> 2. Train your algorithm</a></h4>

<h4 id="-3-asses-your-model"><a href="#test"> 3. Asses your model</a></h4>

<p><a name="intro"></a></p>

<h2 id="what-is-machine-learning">What is Machine Learning?</h2>

<p><strong>Today machine learning is everywhere. From the content delivered to you on your Facebook newsfeed to the spam emails being filtered out of your emails, we live in an increasingly data driven society.</strong></p>

<p>A widely quoted, more formal definition of machine learning is:</p>

<h3 id="a-computer-program-is-said-to-learn-from-experience-e-with-respect-to-some-class-of-tasks-t-and-performance-measure-p-if-its-performance-at-tasks-in-t-as-measured-by-p-improves-with-experience-e">“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.”</h3>

<p><strong>In simple terms, machine Learning is the science of developing and making use of specialised statistical learning algorithms that produce a predictive model based on information gathered from input data.</strong> This is closely related to computational statistics and therefore is far from any wizardry, rather it is based on established methodologies that also come with their flaws. It is therefore important to understand the implications of using off-the-shelf machine learning algorithms when building predictive models to aid knowledge discovery and decision making.</p>

<h3 id="the-k-nearest-neighbours-algorithm-k-nn">The k-nearest neighbours algorithm (<code class="language-plaintext highlighter-rouge">K-nn</code>)</h3>

<p><strong>In this tutorial you will be introduced to a simple and well-established supervised classification algorithm, which we will implement in <code class="language-plaintext highlighter-rouge">R</code>.</strong></p>

<table>
  <thead>
    <tr>
      <th>Main machine learning domains</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Unsupervised_learning" target="_blank" rel="noopener noreferrer">Unsupervised learning</a></td>
      <td>Inferring a function that describes the structure of “unlabeled” data (i.e. data that has not been classified or categorized).</td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" rel="noopener noreferrer">Supervised learning</a></td>
      <td>Learning a function that maps an input to an output based on example input-output pairs.</td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank" rel="noopener noreferrer">Deep learning algorithms</a></td>
      <td>Part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised</td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener noreferrer">Reinforcement learning</a></td>
      <td>An area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.</td>
    </tr>
  </tbody>
</table>

<p><code class="language-plaintext highlighter-rouge">K-nn</code> is an example of a supervised learning method, which means we need to first feed it data so it is able to make a classification based on that data (this is called the training phase). Upon training the algorithm on the data we provided, we can test our model on an unseen dataset (where we know what class each observation belongs to), and can then see how successful our model is at predicting the existing classes. This process of first building or selecting a classifier, training it and subsequently testing it is very widespread across the machine learning field and is what you will be doing today.</p>

<h3 id="under-the-hood">Under the hood</h3>

<p><code class="language-plaintext highlighter-rouge">K-nn</code> is a non-parametric technique that stores all available cases and classifies new cases based on a similiarty measure (distance function). Therefore when classifying an unseen dataset using a trained <code class="language-plaintext highlighter-rouge">K-nn</code> algorithm, it looks through the training data and finds the <strong>k</strong> training examples that are closest to the new example. It then assigns a class label to the new example based on a majority vote between those <strong>k</strong> training examples. This means if <strong>k</strong> is equal to 1, the class label will be assigned based on the nearest neighbour. However if K is equal to 3, the algorithm will select the three closest data points to each case and classify it based on a majority vote based on the classes that those three adjacent points hold.</p>

<center> <img src="https://cambridgecoding.files.wordpress.com/2016/01/knn2.jpg" alt="Img" style="width: 800px;">
</center>
<center>Diagram source: <a href="https://cambridgecoding.wordpress.com" target="_blank" rel="noopener noreferrer">Cambridge Coding</a>
</center>

<p>You can see that the selection of <strong>k</strong> is quite important, as is the selection of your training data, because this is all your predictive model will be based on.
Regarding <strong>k</strong>, generally in binary cases it is best to pick an odd K value to avoid ties between neigbours. Slightly higher <strong>k</strong> values can also act to reduce noise in datasets. However it is best to experiment with different <strong>k</strong> values and use <a href="https://genomicsclass.github.io/book/pages/crossvalidation.html" target="_blank" rel="noopener noreferrer">cross validation techniques</a> to find the best value for your specific case.</p>

<h2 id="getting-started">Getting started</h2>

<p>Today we will use the following packages, go ahead and install them if you haven’t aready, then load them.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of installing a package</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s1">'ggplot2'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Loading required packages for this tutorial</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">class</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">gridExtra</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">gmodels</span><span class="p">)</span><span class="w">

</span><span class="n">devtools</span><span class="o">::</span><span class="n">install_github</span><span class="p">(</span><span class="s1">'cttobin/ggthemr'</span><span class="p">)</span><span class="w">  
</span><span class="c1"># This package is just for setting the colour palette, optional</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggthemr</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="loading-our-data">Loading our data</h3>

<p>For this tutorial we will be using the built-in Iris Machine Learning dataset. In order to start learning something from our data, it is first important that we familiarise ourselves with it first.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Loading iris dataset</span><span class="w">
</span><span class="n">iris.data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">iris</span><span class="w">

</span><span class="c1"># Viewing iris dataset structure and attributes</span><span class="w">
</span><span class="n">str</span><span class="p">(</span><span class="n">iris.data</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>From this we can see that this dataset contains 150 observations describing plant structural traits such as Sepal Length and Petal Width of the Iris genus across three different species.</p>

<h3 id="data-visualisation">Data visualisation</h3>

<p>We can also visualise our data to understand whether there are any apparent trends. Often exploring our data this way will yield an even better understanding of any underlying relationships we may want to explore further using Machine Learning algorithms such as the k-nn.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Set a colour palette</span><span class="w">
</span><span class="n">ggthemr</span><span class="p">(</span><span class="s2">"light"</span><span class="p">)</span><span class="w">  </span><span class="c1"># Optional</span><span class="w">

</span><span class="c1"># Scatter plot visualising petal width and length grouped by species</span><span class="w">
</span><span class="p">(</span><span class="n">scatter</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">iris.data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Petal.Width</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Petal.Length</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Species</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.6</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_classic</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="m">0.8</span><span class="p">)))</span><span class="w">

</span><span class="c1"># Boxplot visualising variation in petal width between species</span><span class="w">
</span><span class="p">(</span><span class="n">boxplot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">iris.data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Species</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Petal.Width</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Species</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_boxplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_classic</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="m">0.8</span><span class="p">)))</span><span class="w">
</span></code></pre></div></div>

<p>Note that putting your entire ggplot code in brackets () creates the graph and then shows it in the plot viewer. If you don’t have the brackets, you’ve only created the object, but haven’t visualized it. You would then have to call the object such that it will be displayed by just typing <code class="language-plaintext highlighter-rouge">barplot</code> after you’ve created the “barplot” object.</p>

<center> <img src="/img/iris_plot1.png" alt="Img" style="width: 500px;">  <img src="/img/iris_plot2.png" alt="Img" style="width: 500px;">
</center>

<p>From the above plots we see a visual correlation between plant traits. We can also see that there is some clustering within species with traits varying greatly between the three iris species. Now that we know that there is a clear difference in structural traits between species we could ask the following question:</p>

<p><a name="train"></a></p>
<h2 id="train-your-algorithm">Train your algorithm</h2>

<h3 id="could-we-predict-what-species-iris-plants-belong-to-based-on-structural-trait-data-alone">Could we predict what species iris plants belong to based on structural trait data alone?</h3>

<p>The goal of this tutorial will be to answer this question by building a predictive model and assessing its performance. To do so we will take a random sample of our data which we will use as training data, and another sample which will be used to test our model. These final predictions can then be compared to our original data so we can assess our results and see how accurate our model is.</p>

<h2 id="building-our-k-nn-classifier">Building our k-nn classifier</h2>

<h3 id="data-normalisation-and-trainingtest-set-generation">Data normalisation and training/test-set generation</h3>

<p>The scales of individual variables may vary with a given dataset. For example one variable may have values ranging from 0 - 1 while the other ranges from 0 - 1000. Therefore some scaling/normalisation is often useful, espescially with the knn algorithm which is quite sensitive to different intervals across variables given that it employs a distance function when searching for ‘nearest-neighbours’.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Building a normalisation function</span><span class="w">
</span><span class="n">normalise</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">num</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
  </span><span class="n">denom</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">num</span><span class="o">/</span><span class="n">denom</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>For further understanding why feature normalisation is useful see <a href="http://www.uta.fi/sis/tie/tl/index/Datamining4.pdf" target="_blank" rel="noopener noreferrer">this lecture</a> and/or a very good <a href="https://stats.stackexchange.com/a/287439" target="_blank" rel="noopener noreferrer">answer</a> on this topic on StackOverflow. Now we normalise all the continous data columns in the iris dataset by applying our function to the iris data.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">iris.norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">lapply</span><span class="p">(</span><span class="n">iris</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">],</span><span class="w"> </span><span class="n">normalise</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Now that our data is normalised, we are going to randomly generate our training and test samples and their respective labels/classes.</p>

<p>This is done using the <code class="language-plaintext highlighter-rouge">sample</code> function which generates a random sample of the specified size from the data set or elements. Also note that before calling <code class="language-plaintext highlighter-rouge">sample</code> we also call the <code class="language-plaintext highlighter-rouge">set.seed</code> function. This ensures that we always generate the same random data sample, as otherwise each time we would run this code a completely new random sequence would be generated. More information on that <a href="http://www.datasciencemadesimple.com/sample-function-in-r/" target="_blank" rel="noopener noreferrer">here</a>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generating seed</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1234</span><span class="p">)</span><span class="w">

</span><span class="c1"># Randomly generating our training and test sampels with a respective ratio of 2/3 and 1/3</span><span class="w">
</span><span class="n">datasample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">iris.norm</span><span class="p">),</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.67</span><span class="p">,</span><span class="w"> </span><span class="m">0.33</span><span class="p">))</span><span class="w">

</span><span class="c1"># Generate training set</span><span class="w">
</span><span class="n">iris.training</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">iris.norm</span><span class="p">[</span><span class="n">datasample</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">]</span><span class="w">

</span><span class="c1"># Generate test set </span><span class="w">
</span><span class="n">iris.test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">iris.norm</span><span class="p">[</span><span class="n">datasample</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>The next step is to generate our training and test labels. Beware however, we now need to use our original (not-normalised) dataset that also includes our class labels (column 5), while in the previous step we were just interested in our continous variables (columns 1-4). If this is not clear view all datasets again before proceeding.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generate training labels</span><span class="w">
</span><span class="n">irisTraining.labels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">iris</span><span class="p">[</span><span class="n">datasample</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">]</span><span class="w">

</span><span class="c1"># Generate test labels</span><span class="w">
</span><span class="n">irisTest.labels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">iris</span><span class="p">[</span><span class="n">datasample</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p><strong>It’s now time to build our classifier! For this we will use the <code class="language-plaintext highlighter-rouge">knn</code> function from the <code class="language-plaintext highlighter-rouge">class</code> package. We will pass the function the following parameters:</strong></p>

<ul>
  <li>Our normalised training dataset</li>
  <li>Our normalised test dataset</li>
  <li>Our original training labels</li>
  <li>A value for K</li>
</ul>

<p>Note that we also select a value for <strong>k</strong>, which in this case is <strong>3</strong>. By chosing an odd value we avoid a tie between the two classes during the algorithm’s majority voting process.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Building our knn classifier</span><span class="w">
</span><span class="n">iris.knn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">knn</span><span class="p">(</span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iris.training</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iris.test</span><span class="p">,</span><span class="w"> </span><span class="n">cl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">irisTraining.labels</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><a name="test"></a></p>
<h2 id="assess-your-model">Assess your model</h2>

<p>Next, we need to evaluate the performance of our model. To do this we want to find out if the classes our algorithm predicts based on the training data accurately predict the species classes in our original iris dataset. For this we compare the original class labels to the predictions made by our algorithm.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># creating a dataframe from known (true) test labels</span><span class="w">
</span><span class="n">test.labels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">irisTest.labels</span><span class="p">)</span><span class="w">

</span><span class="c1"># combining predicted and known species classes</span><span class="w">
</span><span class="n">class.comparison</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">iris.knn</span><span class="p">,</span><span class="w"> </span><span class="n">test.labels</span><span class="p">)</span><span class="w">

</span><span class="c1"># giving appropriate column names</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">class.comparison</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Predicted Species"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Observed Species"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Let’s have a look at how our model did by inspecting the <code class="language-plaintext highlighter-rouge">class.comparison</code> table to see if our predicted species align with our observed species.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># inspecting our results table</span><span class="w">
</span><span class="n">class.comparison</span><span class="w">
</span></code></pre></div></div>

<p>Finally, we can also evaluate the model using a cross-tabulation or so called contingency table. These are very useful when we wish to understand what correlations exist between different categorical variables. In this case we will be able to tell what classes our model predicted and how those predicted classes compare to the actual iris classes.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CrossTable</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">irisTest.labels</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iris.knn</span><span class="p">,</span><span class="w"> </span><span class="n">prop.chisq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                | iris.knn 
irisTest.labels |     setosa | versicolor |  virginica |  Row Total | 
----------------|------------|------------|------------|------------|
         setosa |         12 |          0 |          0 |         12 | 
                |      1.000 |      0.000 |      0.000 |      0.300 | 
                |      1.000 |      0.000 |      0.000 |            | 
                |      0.300 |      0.000 |      0.000 |            | 
----------------|------------|------------|------------|------------|
     versicolor |          0 |         12 |          0 |         12 | 
                |      0.000 |      1.000 |      0.000 |      0.300 | 
                |      0.000 |      0.857 |      0.000 |            | 
                |      0.000 |      0.300 |      0.000 |            | 
----------------|------------|------------|------------|------------|
      virginica |          0 |          2 |         14 |         16 | 
                |      0.000 |      0.125 |      0.875 |      0.400 | 
                |      0.000 |      0.143 |      1.000 |            | 
                |      0.000 |      0.050 |      0.350 |            | 
----------------|------------|------------|------------|------------|
   Column Total |         12 |         14 |         14 |         40 | 
                |      0.300 |      0.350 |      0.350 |            | 
----------------|------------|------------|------------|------------|
</code></pre></div></div>
<p>To evaluate our algorithm’s performance we can check if there are any discrepancies between our <code class="language-plaintext highlighter-rouge">iris.knn</code> model predictions and the actual <code class="language-plaintext highlighter-rouge">irisTest.labels</code>. To do this you can first check the total number of predicted classes per category in the last row under column total.</p>

<p>These can then be compared against the actual classes on the right under row total. Our knn model predicted 12 setosa, 14 versicolor and 14 virginica. However when comparing this to our actual data there were 12 setosa, 12 versicolor and 16 virginca species in our test dataset.</p>

<p>Overall we can see that our algorithm was able to almost predict all species classes correctly, except for a case where two samples where falsely classified as versicolor when in fact they belonged to virginica. To improve the model you could now experiment with using different <code class="language-plaintext highlighter-rouge">k</code> values to see if this impacts your model results in any way.</p>

<p>Finally now that your model is trained you could go ahead and try to implement your algorithm on the entire iris dataset to see how effective it is!</p>

<h3 id="summary-and-next-steps">Summary and Next steps</h3>

<p>In this tutorial we have now covered the following:</p>

<ul>
  <li>the very basics of machine learning in <code class="language-plaintext highlighter-rouge">R</code>
</li>
  <li>implementing a k-nearest neighbour classification algorithm</li>
  <li>building our own training and test datasets</li>
  <li>testing and evaluating our knn algorithm using cross-tabulation</li>
</ul>

<p>However there is still a whole world to explore. For those interested in learning more have a look at this <a href="https://daviddalpiaz.github.io/r4sl/index.html" target="_blank" rel="noopener noreferrer">freely available book</a> on machine learning in R.</p>

<hr>

<hr>

<h3><a href="https://www.surveymonkey.co.uk/r/77YWPQL" target="_blank" rel="noopener noreferrer">  We would love to hear your feedback, please fill out our survey!</a></h3>
<p><br></p>
<h3>  You can contact us with any questions on <a href="mailto:ourcodingclub@gmail.com?Subject=Tutorial%20question" target="_top">ourcodingclub@gmail.com</a>
</h3>
<p><br></p>
<h3>  Related tutorials:</h3>

<p><br></p>
<h3>  Subscribe to our mailing list:</h3>
<div class="container">
	<div class="block">
        <!-- subscribe form start -->
		<div class="form-group">
			<form action="https://getsimpleform.com/messages?form_api_token=de1ba2f2f947822946fb6e835437ec78" method="post">
			<div class="form-group">
				<input type="text" class="form-control" name="Email" placeholder="Email" required="">
			</div>
			<div>
                        	<button class="btn btn-default" type="submit">Subscribe</button>
                    	</div>
                	</form>
		</div>
	</div>
</div>

<ul class="social-icons">
	<li>
		<h3>
			<a href="https://twitter.com/our_codingclub" target="_blank" rel="noopener noreferrer"> Follow our coding adventures on Twitter! <i class="fa fa-twitter"></i></a>
		</h3>
	</li>
</ul>

    	<footer class="footer">
	<hr>
	<div class="footer-container">
    	<ul class="footer-link-list">
        	<li><a href="/tutorials">Tutorials</a></li>
        	<li><a href="/team">About Us</a></li>
        	<li><a href="/contact">Contact us</a></li>
	    	<li><a href="https://twitter.com/our_codingclub" target="_blank" rel="noopener noreferrer">Follow us on Twitter</a></li>
    	</ul>
    	<div class="footer-text">
			<p>We are happy for people to use and further develop our tutorials - please give credit to Coding Club by linking to <a href="https://ourcodingclub.github.io/" target="_blank" rel="noopener noreferrer">our website</a>. We are also happy to discuss possible collaborations, so get in touch at <b>ourcodingclub@gmail.com</b></p>
    		<p>See our <a href="/terms">Terms of Use</a> and our <a href="/privacy">Data Privacy policy</a>.</p>
			<p>This work is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener noreferrer">Creative Commons Attribution-ShareAlike 4.0 International License</a></p>
			<a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener noreferrer"><img class="license" src="https://licensebuttons.net/l/by-sa/4.0/80x15.png" alt="CC-by-sa-4.0"></a>
    	</div>
    </div>
</footer>

<!-- JS -->
<script src="/scripts/owl.carousel.js"></script>
<script src="/scripts/owl.carousel-init.js"></script>
<script src="/scripts/reveal.js"></script>


	
	</body>
</html>
